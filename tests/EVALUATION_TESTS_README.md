# Evaluation Package Tests

This directory contains comprehensive tests for the evaluation package to ensure your refactoring doesn't break existing functionality.

## ğŸ“ Test Files

- `test_evaluation_schemas.py` - Tests for data schemas (Message, Conversation)
- `test_evaluation_utils.py` - Tests for utility functions (calc_md5, validation, etc.)
- `test_evaluation_execute.py` - Tests for execution logic (exec_agent function)
- `test_evaluation_bulk_tasks.py` - Tests for bulk task evaluation functionality
- `test_evaluation_gen_variants.py` - Tests for variant generation logic
- `run_evaluation_tests.py` - Main test runner script
- `validate_refactor.py` - Pre/post refactor validation script

## ğŸš€ Quick Start

### Before Refactoring

1. **Run all tests to establish baseline:**
   ```bash
   cd tests
   python run_evaluation_tests.py
   ```

2. **Capture pre-refactor state:**
   ```bash
   python validate_refactor.py before
   ```

### After Refactoring

3. **Validate your refactor:**
   ```bash
   python validate_refactor.py after
   ```

4. **Compare results:**
   ```bash
   python validate_refactor.py compare
   ```

## ğŸ§ª Individual Test Commands

### Run specific test file:
```bash
python run_evaluation_tests.py --file test_evaluation_schemas.py
```

### Run quick smoke test:
```bash
python run_evaluation_tests.py --smoke
```

### Run with pytest directly:
```bash
pytest test_evaluation_schemas.py -v
pytest test_evaluation_utils.py -v
pytest test_evaluation_execute.py -v
pytest test_evaluation_bulk_tasks.py -v
pytest test_evaluation_gen_variants.py -v
```

## ğŸ“Š Understanding Test Results

### âœ… Success Indicators
- All tests pass (green checkmarks)
- No import errors
- All assertions pass

### âŒ Failure Indicators
- Red X marks or FAILED status
- Import errors (missing dependencies)
- Assertion failures
- Runtime exceptions

### âš ï¸ Warnings to Watch
- Deprecated function usage
- Missing optional dependencies (NLTK, Levenshtein)
- Configuration issues

## ğŸ”§ Test Coverage

### Schemas Module (`test_evaluation_schemas.py`)
- âœ… Message creation and validation
- âœ… Conversation structure testing
- âœ… Tool call handling
- âœ… Multimodal content support
- âœ… Date formatting validation

### Utils Module (`test_evaluation_utils.py`)
- âœ… MD5 hash calculation consistency
- âœ… Dataset folder validation
- âœ… Supabase integration (mocked)
- âœ… Error handling
- âœ… Unicode support

### Execute Module (`test_evaluation_execute.py`)
- âœ… Agent execution logic
- âœ… Parameter validation
- âœ… Authentication handling
- âœ… Save vs evaluate modes
- âœ… Error scenarios

### Bulk Tasks Module (`test_evaluation_bulk_tasks.py`)
- âœ… Database configuration
- âœ… CSV processing
- âœ… Template formatting
- âœ… API integration (mocked)
- âœ… Task generation logic

### Gen Variants Module (`test_evaluation_gen_variants.py`)
- âœ… Text transformation
- âœ… Variant generation algorithms
- âœ… File operations
- âœ… Metrics calculation
- âœ… Optional dependency handling

## ğŸ Python Environment Requirements

Make sure you have the required packages:

```bash
pip install pytest pandas requests psycopg2-binary
```

Optional packages (for full functionality):
```bash
pip install nltk python-levenshtein
```

## ğŸ—ï¸ Refactoring Workflow

1. **Establish Baseline** ğŸ“‹
   ```bash
   python validate_refactor.py before
   ```

2. **Perform Your Refactoring** ğŸ”¨
   - Refactor your evaluation code
   - Maintain public API compatibility
   - Keep the same functionality

3. **Validate Changes** âœ…
   ```bash
   python validate_refactor.py after
   python validate_refactor.py compare
   ```

4. **Review Results** ğŸ“Š
   - Check `tests/refactor_validation_results/` for detailed reports
   - Ensure all tests still pass
   - Review any performance changes

## ğŸš¨ Common Issues and Solutions

### Import Errors
- **Problem**: `ModuleNotFoundError: No module named 'evaluation.schemas'`
- **Solution**: Make sure you're running from the correct directory and Python path is set

### Missing Dependencies
- **Problem**: Tests skip due to missing packages (NLTK, Levenshtein)
- **Solution**: Install optional dependencies or ignore skipped tests

### Database Connection Errors
- **Problem**: PostgreSQL connection failures in bulk tasks tests
- **Solution**: Tests use mocking - actual DB not needed. Check mock configuration.

### Timeout Issues
- **Problem**: Tests take too long or timeout
- **Solution**: Use `--quick` flag or check for infinite loops in your code

## ğŸ“ˆ Test Metrics

The validation script tracks:
- âœ… **Pass/Fail Status** - Whether tests pass before/after
- â±ï¸ **Execution Time** - Performance impact of refactoring
- ğŸ” **Detailed Logs** - Full output for debugging
- ğŸ“Š **Comparison Report** - Side-by-side analysis

## ğŸ¯ Success Criteria

Your refactor is successful when:
1. âœ… All tests that passed before still pass after
2. âœ… No new critical errors introduced
3. âœ… Performance doesn't significantly degrade
4. âœ… All public APIs remain functional

## ğŸ“ Troubleshooting

If tests fail after refactoring:

1. **Check the comparison report** in `refactor_validation_results/`
2. **Run individual test files** to isolate issues
3. **Compare before/after logs** to identify breaking changes
4. **Use pytest's verbose mode** for detailed error information

```bash
pytest test_evaluation_schemas.py -v -s --tb=long
```

Good luck with your refactoring! ğŸš€
